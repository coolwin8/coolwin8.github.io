{"meta":{"title":"CoolWin8的技术博客","subtitle":"Notes","description":"coolwin8的日常技术笔记","author":"lengwei","url":"http://coolwin8.github.io","root":"/"},"pages":[{"title":"categories","date":"2020-04-11T12:19:56.000Z","updated":"2020-04-11T04:20:26.880Z","comments":true,"path":"categories/index.html","permalink":"http://coolwin8.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"利用softEther VPN远程访问内部网络","slug":"利用softEther VPN远程访问内部网络","date":"2020-04-14T12:39:53.000Z","updated":"2020-04-13T16:10:49.215Z","comments":true,"path":"2020/04/14/利用softEther VPN远程访问内部网络/","link":"","permalink":"http://coolwin8.github.io/2020/04/14/%E5%88%A9%E7%94%A8softEther%20VPN%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AE%E5%86%85%E9%83%A8%E7%BD%91%E7%BB%9C/","excerpt":"","text":"利用 softEther VPN 进行远程访问1作者: whr 使用场景和前提条件 内部网络受到保护，无法直接在内部网络中的主机中直接部署VPN server（搭建完无法连接到该server） 用户需要从外部网络远程访问内部网络中的资源，如主机或打印机等等 对内部网络中的某台主机拥有管理权限 有一台暴露在互联网中的主机 内部网络中的主机可以直接访问互联网 比如 学生于校外访问校园网资源 远程办公 管理家庭局域网设备 举个栗子这里通过一个例子，描述如何通过一步步的操作，利用softEther VPN 套件实现远程访问。 一个安全的网络（192.168.1.0/24），它受到防火墙和NAT的保护，无法从Internet访问，但是可以通过防火墙或NAT代理访问Internet上的网站。该网络中， IP地址为 192.168.1.111 的一台打印机，IP地址为 192.186.1.222 的一台主机H。一台暴露在Internet的主机S，IP地址为 125.111.111.111。 目标 通过构建主机S和主机H所在内部网络的级联网络，用户可以通过连接主机S所创建的VPN，实现对内部网络（192.168.1.0/24）中的资源的访问。 开始你的表演总体上整个任务的关键位置所运行的SoftEther VPN组件如下图所示 1、在主机S上安装搭建VPN server连接到 主机S ，将SoftEther VPN Server 组件下载到本地。组件选择地址根据实际运行环境，选择适当的版本。这里我选择的是Linux 系统，CPU选择的是Intel x64 / AMD64 (64bit)。 拿到下载地址后，使用wget命令进行下载，： 12345678# 下载softether-vpnserver到本地wget https:&#x2F;&#x2F;github.com&#x2F;SoftEtherVPN&#x2F;SoftEtherVPN_Stable&#x2F;releases&#x2F;download&#x2F;v4.30-9696-beta&#x2F;softether-vpnserver-v4.30-9696-beta-2019.07.08-linux-x64-64bit.tar.gz#解压tar zxvf softether-*server*.tar.gzcd vpnserver&#x2F;# 这里面有一个隐藏的文件.install.sh，运行它。[vpnserver]# .&#x2F;.install.sh# 一路Yes，Agree默认即可 查看下载的文件进入对应文件夹运行安装脚本进行默认配置 安装完成后进入 1234567891011121314151617181920212223242526272829# 启动vpnserver[vpnserver]# .&#x2F;vpnserver start# 进行vpn的初始配置[vpnserver]# .&#x2F; vpncmdBy using vpncmd program, the following can be achieved.1. Management of VPN Server or VPN Bridge2. Management of VPN Client3. Use of VPN Tools (certificate creation and Network Traffic Speed Test Tool)Select 1, 2 or 3:# 选择1 回车Hostname of IP Address of Destination:# 回车If connecting to the server by Virtual Hub Admin Mode, please input the Virtual Hub name.If connecting by server admin mode, please press Enter without inputting anything.Specify Virtual Hub Name:# 回车Password:# 回车VPN Server&gt;ServerPasswordSet# 输入 ServerPasswordSetServerPasswordSet command - Set VPN Server Administrator PasswordPlease enter the password. To cancel press the Ctrl+D key.# 设置你的服务器管理密码Password: ******#这里输入管理密码Confirm input: ******#确认密码 至此，主机S上的vpnserver的安装和初步的配置就完成了，后面将介绍使用SoftEther VPN Server Manager 对其进行进一步的配置。 2、在主机H上安装搭建VPN Bridge连接到 主机H ，将SoftEther VPN Bridge 组件下载到本地。这里就不贴图了，具体和下载vpnserver一样，换成vpnbridge。下载地址 根据实际运行环境，选择适当的版本。这里我选择的是Linux 系统，CPU选择的是Intel x64 / AMD64 (64bit)。使用wget 命令进行下载，安装，初步配置也都相近： 12345678910111213141516171819202122232425262728293031323334353637383940# 下载softether-vpnbridge到本地wget https:&#x2F;&#x2F;github.com&#x2F;SoftEtherVPN&#x2F;SoftEtherVPN_Stable&#x2F;releases&#x2F;download&#x2F;v4.30-9696-beta&#x2F;softether-vpnbridge-v4.30-9696-beta-2019.07.08-linux-x64-64bit.tar.gz#解压tar zxvf softether-*bridge*.tar.gzcd vpnserver&#x2F;# 这里面有一个隐藏的文件.install.sh，运行它。[vpnbridge]# .&#x2F;.install.sh# 一路Yes，Agree默认即可[vpnbridge]# .&#x2F;vpnbridge start[vpnbridge]# .&#x2F;vpncmd...By using vpncmd program, the following can be achieved.1. Management of VPN Server or VPN Bridge2. Management of VPN Client3. Use of VPN Tools (certificate creation and Network Traffic Speed Test Tool)Select 1, 2 or 3: 1# 输入1Specify the host name or IP address of the computer that the destination VPN Server or VPN Bridge is operating on.By specifying according to the format &#39;host name:port number&#39;, you can also specify the port number.(When the port number is unspecified, 443 is used.)If nothing is input and the Enter key is pressed, the connection will be made to the port number 8888 of localhost (this computer).Hostname of IP Address of Destination:#回车If connecting to the server by Virtual Hub Admin Mode, please input the Virtual Hub name.If connecting by server admin mode, please press Enter without inputting anything.Specify Virtual Hub Name:#回车Connection has been established with VPN Server &quot;localhost&quot; (port 443).You have administrator privileges for the entire VPN Server.VPN Server&gt;ServerPasswordSetServerPasswordSet command - Set VPN Server Administrator PasswordPlease enter the password. To cancel press the Ctrl+D key.Password: ******#这里输入管理密码Confirm input: ******#确认密码The command completed successfully. 3、配置主机S上的vpnserver这里需要找了一台windows的机子，因为SoftEther提供了一款SoftEther VPN Server Manager的图形管理器，进行蛇者比较方便。要求就是最好在内网中，方便连接主机H，同时也能连接主机S。将SoftEther VPN Server Manager for Windows 图形组件下载到本地。下载地址 运行SoftEther VPN Server Manager，点击设置 填入设置名（这个随意），主机名填入主机S的IP地址，端口采用默认的443，代理类型选择直接TCP/IP连接（无代理），选中服务端管理模式并填入前面设置的vpnserver管理密码，点击确定。如果你在前面没有设置管理密码，在连接的时候将也提示你设置密码。 勾选远程访问VPN Server (R)，点击下一步后提示确认初始化，选择 是。 为你的vpnserver设置一个名字，这里输入VPN，点击确定。 动态DNS功能，不用管，直接无视选择退出。 IPsec/L2TP 设置，勾选启用L2TP服务器功能，并设置IPsec预共享秘钥。这一步是为了后面我们可以用手机或者PC自带的VPN工具进行连接。 VPN Azure 云，勾选禁用，点击确定。 创建一个用户来接受VPN连接，点击创建用户。 填入相关信息，验证类型选择密码验证，并在密码验证设置中填入密码。 我们这里需要创建两个账号，分别为user1和user2，一个用于主机H上的VPN bridge，一个用于用户在外网的状态下登录VPN。 点击关闭进入下一步。 点击管理HUB。 点击虚拟NAT和虚拟DHCP服务器。 点击启用SecureNAT，这一步很关键。 确定启用SecureNAT。 点击关闭。 至此，我们对于主机S上的VPN server的配置就完成了，接下来连接到主机H上的vpn bridge进行配置。 4、配置主机H上的vpnbridge同样是回到SoftEther VPN Server Manager主界面，点击新设置，填入VPN bridge的IP地址，这里是 192.168.1.222，如果前面在初步配置的时候选择默认的话，端口选择5555，如果前面在初步配置的时候选择默认的话。并填入管理密码。（参考3-1,3-2） 勾选站到站 VPN Server 或 VPN Bridge，选择下一步。 设置本地网桥。这里如果你有多张网卡的话，要选择你想级联出去网段所在的网卡。这里我想级联192.168.1.0/24网段，我选择配置为该网段地址的网卡，eth0。不清楚对应网卡的可以在控制台运行ifconfig命令查看。 回到管理器面板，点击管理虚拟HUB，点击虚拟NAT和虚拟DHCP服务器，启用 SecureNAT。 回到管理器面板，点击管理虚拟HUB，选择管理级联连接。 填写连接设置名(自由设置)，填写vpnserver的地址，即主机S的IP地址，端口号选择443。只要你的地址和端口填写正确，并且前面配置无误，这里虚拟HUB名的下拉列表就能找到我们之前创建的虚拟HUB “VPN”。勾选无代理，选择认证类型为标准密码验证，并填写用户名密码。这里使用创建的用户user1。点击确定。 进入到级联管理界面，选中我们刚刚创建的连接，点击在线。 发现该装填已由离线变成在线。 至此，我们在对主机H的vpn bridge的配置也大功告成。此时主机S与主机H逻辑上已经是同属一个网络，主机S可以直接访问内网192.168.1.0/24网络。所以下一步，我们只要连接到主机S的VPN，我们也就可以和主机S一样，直接访问内网192.168.1.0/24网络的资源了。 5. 用户配置L2TP/IPsec VPN IOS端设置。 Android端设置。 ————————————————连接时填入用户名密码即可。 Windows端设置。 连接有问题的时候请检查一下主机S的防火墙和安全策略设置。请开启L2TP必要的端口：UDP 500、4500、1701。 完结撒花最后更新： 2019年11月03日 09:51 原始链接： https://silence-linhl.github.io/blog/2019/11/02/softEther/ Harlan","categories":[{"name":"系统应用 网络应用","slug":"系统应用-网络应用","permalink":"http://coolwin8.github.io/categories/%E7%B3%BB%E7%BB%9F%E5%BA%94%E7%94%A8-%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8/"}],"tags":[{"name":"网络配置","slug":"网络配置","permalink":"http://coolwin8.github.io/tags/%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE/"},{"name":"VPN","slug":"VPN","permalink":"http://coolwin8.github.io/tags/VPN/"},{"name":"内网访问","slug":"内网访问","permalink":"http://coolwin8.github.io/tags/%E5%86%85%E7%BD%91%E8%AE%BF%E9%97%AE/"}]},{"title":"maven连接nexus私服配置文件详解","slug":"maven连接nexus私服配置文件详解","date":"2020-04-13T12:39:53.000Z","updated":"2020-04-13T04:46:22.321Z","comments":true,"path":"2020/04/13/maven连接nexus私服配置文件详解/","link":"","permalink":"http://coolwin8.github.io/2020/04/13/maven%E8%BF%9E%E6%8E%A5nexus%E7%A7%81%E6%9C%8D%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"settings.xml有什么用？ 如果在Eclipse中使用过Maven插件，想必会有这个经验：配置settings.xml文件的路径。 settings.xml文件是干什么的，为什么要配置它呢？从settings.xml的文件名就可以看出，它是用来设置maven参数的配置文件。并且，settings.xml是maven的全局配置文件。而pom.xml文件是所在项目的局部配置。Settings.xml中包含类似本地仓储位置、修改远程仓储服务器、认证信息等配置。 settings.xml文件位置 settings.xml文件一般存在于两个位置：全局配置: ${M2_HOME}/conf/settings.xml用户配置: {user.home} 和和所有其他系统属性只能在3.0+版本上使用。请注意windows和Linux使用变量的区别。 配置优先级 需要注意的是：局部配置优先于全局配置。配置优先级从高到低：pom.xml&gt; user settings &gt; global settings如果这些文件同时存在，在应用配置时，会合并它们的内容，如果有重复的配置，优先级高的配置会覆盖优先级低的。 maven怎么从远程仓库下载jar包 setting中配置： &lt;!-- 我们使用maven下载需要的jar包，但是很多的时候由于中央仓库没有,所以此处可以在maven的设置中心添加多个下载仓库，当中央仓库没有的话，继续到下一个仓库去下载。这样丰富了中央仓库的下载地址。 --&gt; &lt;mirror&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;nexus maven&lt;/name&gt; &lt;url&gt;http://localhost:8081/repository/maven-public/&lt;/url&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;repo2&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt; &lt;url&gt;http://repo2.maven.org/maven2/&lt;/url&gt; &lt;/mirror&gt; &lt;/mirrors&gt;Nexus配置项目使用nexus私服的jar包，在项目的pom.xml文件中指定私服仓库 12345678910111213&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;&#x2F;id&gt; &lt;name&gt;nexus&lt;&#x2F;name&gt; &lt;url&gt;http:&#x2F;&#x2F;192.168.1.103:8081&#x2F;nexus&#x2F;content&#x2F;groups&#x2F;public&#x2F;&lt;&#x2F;url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;&#x2F;enabled&gt; &lt;&#x2F;releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;&#x2F;enabled&gt; &lt;&#x2F;snapshots&gt; &lt;&#x2F;repository&gt; &lt;&#x2F;repositories&gt; 项目使用nexus私服的插件，在项目的pom.xml文件中指定插件仓库 12345678910111213&lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus&lt;&#x2F;id&gt; &lt;name&gt;nexus&lt;&#x2F;name&gt; &lt;url&gt;http:&#x2F;&#x2F;192.168.1.103:8081&#x2F;nexus&#x2F;content&#x2F;groups&#x2F;public&#x2F;&lt;&#x2F;url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;&#x2F;enabled&gt; &lt;&#x2F;releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;&#x2F;enabled&gt; &lt;&#x2F;snapshots&gt; &lt;&#x2F;pluginRepository&gt; &lt;&#x2F;pluginRepositories&gt; 如果想本机所有的maven项目都使用私服的组件，可以在maven的设置文件settings.xml中添加属性，并激活 12345678910111213141516171819202122232425262728&lt;profile&gt; &lt;id&gt;Nexus&lt;&#x2F;id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;&#x2F;id&gt; &lt;url&gt;http:&#x2F;&#x2F;localhost:8081&#x2F;repository&#x2F;maven-public&#x2F;&lt;&#x2F;url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;&#x2F;enabled&gt;&lt;&#x2F;releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;&#x2F;enabled&gt;&lt;&#x2F;snapshots&gt; &lt;&#x2F;repository&gt; &lt;&#x2F;repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus&lt;&#x2F;id&gt; &lt;url&gt;http:&#x2F;&#x2F;localhost:8081&#x2F;repository&#x2F;maven-public&#x2F;&lt;&#x2F;url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;&#x2F;enabled&gt; &lt;&#x2F;releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;&#x2F;enabled&gt; &lt;&#x2F;snapshots&gt; &lt;&#x2F;pluginRepository&gt; &lt;&#x2F;pluginRepositories&gt; &lt;&#x2F;profile&gt; &lt;&#x2F;profiles&gt; &lt;!-- 激活 --&gt; &lt;activeProfiles&gt; &lt;activeProfile&gt;Nexus&lt;&#x2F;activeProfile&gt; &lt;&#x2F;activeProfiles&gt; 如何将自己的项目发布到nexus私服 我们知道用mvn install命令可以将项目装载的本地的仓库，但是项目发布到私服，maven项目就要使用命令：mvn clean deploy；要想发布项目到nexus里，必须通过标签来进行配置。在之前的文章中有介绍nexus的工厂类别，其中提到两个：hosted里的Releases、Snapshots. 当我们发布项目到nexus里时，如果项目版本是x.x.x-Releases，则会发布到Releases工厂中；而项目版本是x.x.x-SNAPSHOTS则发布到Snapshots工厂中。需要在pom文件中配置一下代码； 123456789101112&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;nexus-releases&lt;&#x2F;id&gt; &lt;name&gt;Nexus Release Repository&lt;&#x2F;name&gt; &lt;url&gt;http:&#x2F;&#x2F;localhost:8081&#x2F;repository&#x2F;maven-releases&#x2F;&lt;&#x2F;url&gt; &lt;&#x2F;repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;nexus-snapshots&lt;&#x2F;id&gt; &lt;name&gt;Nexus Snapshot Repository&lt;&#x2F;name&gt; &lt;url&gt;http:&#x2F;&#x2F;localhost:8081&#x2F;repository&#x2F;maven-snapshots&#x2F;&#x2F;&lt;&#x2F;url&gt; &lt;&#x2F;snapshotRepository&gt; &lt;&#x2F;distributionManagement&gt; 注意还需要配置mvn发布的权限，否则会报401错误，在settings.xml中配置权限，其中id要与pom文件中的id一致 1234567891011&lt;!--授权信息 --&gt; &lt;server&gt; &lt;id&gt;nexus-releases&lt;&#x2F;id&gt; &lt;username&gt;admin&lt;&#x2F;username&gt; &lt;password&gt;admin123&lt;&#x2F;password&gt; &lt;&#x2F;server&gt; &lt;server&gt; &lt;id&gt;nexus-snapshots&lt;&#x2F;id&gt; &lt;username&gt;admin&lt;&#x2F;username&gt; &lt;password&gt;admin123&lt;&#x2F;password&gt; &lt;&#x2F;server&gt; 这里面的username和password对应的是nexus私服中具有发布权限的用户名和密码","categories":[{"name":"java maven","slug":"java-maven","permalink":"http://coolwin8.github.io/categories/java-maven/"}],"tags":[{"name":"配置管理","slug":"配置管理","permalink":"http://coolwin8.github.io/tags/%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86/"},{"name":"nexus","slug":"nexus","permalink":"http://coolwin8.github.io/tags/nexus/"},{"name":"maven","slug":"maven","permalink":"http://coolwin8.github.io/tags/maven/"}]},{"title":"机器学习如何选择特征","slug":"机器学习如何选择特征","date":"2020-04-11T06:11:56.583Z","updated":"2020-04-11T06:23:54.817Z","comments":true,"path":"2020/04/11/机器学习如何选择特征/","link":"","permalink":"http://coolwin8.github.io/2020/04/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E7%89%B9%E5%BE%81/","excerpt":"","text":"看到一篇好文章分享出来，看别人是如何选特征的，作者是Edwin Jarvis 作者：Edwin Jarvis 特征选择(排序)对于数据科学家、机器学习从业者来说非常重要。好的特征选择能够提升模型的性能，更能帮助我们理解数据的特点、底层结构，这对进一步改善模型、算法都有着重要作用。 特征选择主要有两个功能： 减少特征数量、降维，使模型泛化能力更强，减少过拟合 增强对特征和特征值之间的理解 拿到数据集，一个特征选择方法，往往很难同时完成这两个目的。通常情况下，我们经常不管三七二十一，选择一种自己最熟悉或者最方便的特征选择方法（往往目的是降维，而忽略了对特征和数据理解的目的）。 在许多机器学习相关的书里，很难找到关于特征选择的内容，因为特征选择要解决的问题往往被视为机器学习的一种副作用，一般不会单独拿出来讨论。 本文将结合Scikit-learn提供的例子介绍几种常用的特征选择方法，它们各自的优缺点和问题。 1 去掉取值变化小的特征 Removing features with low variance这应该是最简单的特征选择方法了：假设某特征的特征值只有0和1，并且在所有输入样本中，95%的实例的该特征取值都是1，那就可以认为这个特征作用不大。如果100%都是1，那这个特征就没意义了。当特征值都是离散型变量的时候这种方法才能用，如果是连续型变量，就需要将连续变量离散化之后才能用，而且实际当中，一般不太会有95%以上都取某个值的特征存在，所以这种方法虽然简单但是不太好用。可以把它作为特征选择的预处理，先去掉那些取值变化小的特征，然后再从接下来提到的的特征选择方法中选择合适的进行进一步的特征选择。 2 单变量特征选择 Univariate feature selection单变量特征选择能够对每一个特征进行测试，衡量该特征和响应变量之间的关系，根据得分扔掉不好的特征。对于回归和分类问题可以采用卡方检验等方式对特征进行测试。 这种方法比较简单，易于运行，易于理解，通常对于理解数据有较好的效果（但对特征优化、提高泛化能力来说不一定有效）；这种方法有许多改进的版本、变种。 2.1 Pearson相关系数 Pearson Correlation皮尔森相关系数是一种最简单的，能帮助理解特征和响应变量之间关系的方法，该方法衡量的是变量之间的==线性\b相关性==，结果的取值区间为[-1，1]，-1表示完全的负相关(这个变量下降，那个就会上升)，+1表示完全的正相关，0表示没有线性相关。 Pearson Correlation速度快、易于计算，经常在拿到数据(经过清洗和特征提取之后的)之后第一时间就执行。Scipy的pearsonr方法能够同时计算相关系数和p-value， 12345678import numpy as npfrom scipy.stats import pearsonrnp.random.seed(0)size &#x3D; 300x &#x3D; np.random.normal(0, 1, size)print &quot;Lower noise&quot;, pearsonr(x, x + np.random.normal(0, 1, size))print &quot;Higher noise&quot;, pearsonr(x, x + np.random.normal(0, 10, size)) Lower noise (0.71824836862138386, 7.3240173129992273e-49)Higher noise (0.057964292079338148, 0.31700993885324746) 这个例子中，我们比较了变量在加入噪音之前和之后的差异。当噪音比较小的时候，相关性很强，p-value很低。 Scikit-learn提供的f_regrssion方法能够批量计算特征的p-value，非常方便，参考sklearn的pipeline Pearson相关系数的一个明显缺陷是，作为特征排序机制，他只对线性关系敏感。如果关系是非线性的，即便两个变量具有一一对应的关系，Pearson相关性也可能会接近0。 12x &#x3D; np.random.uniform(-1, 1, 100000)print pearsonr(x, x**2)[0] -0.00230804707612 更多类似的例子参考sample plots。另外，如果仅仅根据相关系数这个值来判断的话，有时候会具有很强的误导性，如Anscombe’s quartet，最好把数据可视化出来，以免得出错误的结论。 2.2 互信息和最大信息系数 Mutual information and maximal information coefficient (MIC)以上就是经典的互信息公式了。想把互信息直接用于特征选择其实不是太方便：1、它不属于度量方式，也没有办法归一化，在不同数据及上的结果无法做比较；2、对于连续变量的计算不是很方便（X和Y都是集合，x，y都是离散的取值），通常变量需要先离散化，而互信息的结果对离散化的方式很敏感。 最大信息系数克服了这两个问题。它首先寻找一种最优的离散化方式，然后把互信息取值转换成一种度量方式，取值区间在[0，1]。minepy提供了MIC功能。 反过头来看y=x^2这个例子，MIC算出来的互信息值为1(最大的取值)。 123456from minepy import MINEm &#x3D; MINE()x &#x3D; np.random.uniform(-1, 1, 10000)m.compute_score(x, x**2)print m.mic() 1.0 MIC的统计能力遭到了一些质疑，当零假设不成立时，MIC的统计就会受到影响。在有的数据集上不存在这个问题，但有的数据集上就存在这个问题。 2.3 距离相关系数 (Distance correlation)距离相关系数是为了克服Pearson相关系数的弱点而生的。在x和x^2这个例子中，即便Pearson相关系数是0，我们也不能断定这两个变量是独立的（有可能是非线性相关）；但如果距离相关系数是0，那么我们就可以说这两个变量是独立的。 R的energy包里提供了距离相关系数的实现，另外这是Python gist的实现。 1234#R-code&gt; x &#x3D; runif (1000, -1, 1)&gt; dcor(x, x**2)[1] 0.4943864 尽管有MIC和距离相关系数在了，但当变量之间的关系接近线性相关的时候，Pearson相关系数仍然是不可替代的。第一、Pearson相关系数计算速度快，这在处理大规模数据的时候很重要。第二、Pearson相关系数的取值区间是[-1，1]，而MIC和距离相关系数都是[0，1]。这个特点使得Pearson相关系数能够表征更丰富的关系，符号表示关系的正负，绝对值能够表示强度。当然，Pearson相关性有效的前提是两个变量的变化关系是单调的。 2.4 基于学习模型的特征排序 (Model based ranking)这种方法的思路是直接使用你要用的机器学习算法，针对每个单独的特征和响应变量建立预测模型。其实Pearson相关系数等价于线性回归里的标准化回归系数。假如某个特征和响应变量之间的关系是非线性的，可以用基于树的方法（决策树、随机森林）、或者扩展的线性模型等。基于树的方法比较易于使用，因为他们对非线性关系的建模比较好，并且不需要太多的调试。但要注意过拟合问题，因此树的深度最好不要太大，再就是运用交叉验证。 在波士顿房价数据集上使用sklearn的随机森林回归给出一个单变量选择的例子： 1234567891011121314151617from sklearn.cross_validation import cross_val_score, ShuffleSplitfrom sklearn.datasets import load_bostonfrom sklearn.ensemble import RandomForestRegressor#Load boston housing dataset as an exampleboston &#x3D; load_boston()X &#x3D; boston[&quot;data&quot;]Y &#x3D; boston[&quot;target&quot;]names &#x3D; boston[&quot;feature_names&quot;]rf &#x3D; RandomForestRegressor(n_estimators&#x3D;20, max_depth&#x3D;4)scores &#x3D; []for i in range(X.shape[1]): score &#x3D; cross_val_score(rf, X[:, i:i+1], Y, scoring&#x3D;&quot;r2&quot;, cv&#x3D;ShuffleSplit(len(X), 3, .3)) scores.append((round(np.mean(score), 3), names[i]))print sorted(scores, reverse&#x3D;True) [(0.636, ‘LSTAT’), (0.59, ‘RM’), (0.472, ‘NOX’), (0.369, ‘INDUS’), (0.311, ‘PTRATIO’), (0.24, ‘TAX’), (0.24, ‘CRIM’), (0.185, ‘RAD’), (0.16, ‘ZN’), (0.087, ‘B’), (0.062, ‘DIS’), (0.036, ‘CHAS’), (0.027, ‘AGE’)] 3 线性模型和正则化单变量特征选择方法独立的衡量每个特征与响应变量之间的关系，另一种主流的特征选择方法是基于机器学习模型的方法。有些机器学习方法本身就具有对特征进行打分的机制，或者很容易将其运用到特征选择任务中，例如回归模型，SVM，决策树，随机森林等等。说句题外话，这种方法好像在一些地方叫做wrapper类型，大概意思是说，特征排序模型和机器学习模型是耦盒在一起的，对应的非wrapper类型的特征选择方法叫做filter类型。 下面将介绍如何用回归模型的系数来选择特征。越是重要的特征在模型中对应的系数就会越大，而跟输出变量越是无关的特征对应的系数就会越接近于0。在噪音不多的数据上，或者是数据量远远大于特征数的数据上，如果特征之间相对来说是比较独立的，那么即便是运用最简单的线性回归模型也一样能取得非常好的效果。 123456789101112131415161718192021222324from sklearn.linear_model import LinearRegressionimport numpy as npnp.random.seed(0)size &#x3D; 5000#A dataset with 3 featuresX &#x3D; np.random.normal(0, 1, (size, 3))#Y &#x3D; X0 + 2*X1 + noiseY &#x3D; X[:,0] + 2*X[:,1] + np.random.normal(0, 2, size)lr &#x3D; LinearRegression()lr.fit(X, Y)#A helper method for pretty-printing linear modelsdef pretty_print_linear(coefs, names &#x3D; None, sort &#x3D; False): if names &#x3D;&#x3D; None: names &#x3D; [&quot;X%s&quot; % x for x in range(len(coefs))] lst &#x3D; zip(coefs, names) if sort: lst &#x3D; sorted(lst, key &#x3D; lambda x:-np.abs(x[0])) return &quot; + &quot;.join(&quot;%s * %s&quot; % (round(coef, 3), name) for coef, name in lst)print &quot;Linear model:&quot;, pretty_print_linear(lr.coef_) Linear model: 0.984 * X0 + 1.995 * X1 + -0.041 * X2 在这个例子当中，尽管数据中存在一些噪音，但这种特征选择模型仍然能够很好的体现出数据的底层结构。当然这也是因为例子中的这个问题非常适合用线性模型来解：特征和响应变量之间全都是线性关系，并且特征之间均是独立的。 在很多实际的数据当中，往往存在多个互相关联的特征，这时候模型就会变得不稳定，数据中细微的变化就可能导致模型的巨大变化（模型的变化本质上是系数，或者叫参数，可以理解成W），这会让模型的预测变得困难，这种现象也称为多重共线性。例如，假设我们有个数据集，它的真实模型应该是Y=X1+X2，当我们观察的时候，发现Y’=X1+X2+e，e是噪音。如果X1和X2之间存在线性关系，例如X1约等于X2，这个时候由于噪音e的存在，我们学到的模型可能就不是Y=X1+X2了，有可能是Y=2X1，或者Y=-X1+3X2。 下边这个例子当中，在同一个数据上加入了一些噪音，用随机森林算法进行特征选择。 12345678910111213141516from sklearn.linear_model import LinearRegressionsize &#x3D; 100np.random.seed(seed&#x3D;5)X_seed &#x3D; np.random.normal(0, 1, size)X1 &#x3D; X_seed + np.random.normal(0, .1, size)X2 &#x3D; X_seed + np.random.normal(0, .1, size)X3 &#x3D; X_seed + np.random.normal(0, .1, size)Y &#x3D; X1 + X2 + X3 + np.random.normal(0,1, size)X &#x3D; np.array([X1, X2, X3]).Tlr &#x3D; LinearRegression()lr.fit(X,Y)print &quot;Linear model:&quot;, pretty_print_linear(lr.coef_) Linear model: -1.291 * X0 + 1.591 * X1 + 2.747 * X2 系数之和接近3，基本上和上上个例子的结果一致，应该说学到的模型对于预测来说还是不错的。但是，如果从系数的字面意思上去解释特征的重要性的话，X3对于输出变量来说具有很强的正面影响，而X1具有负面影响，而实际上所有特征与输出变量之间的影响是均等的。 同样的方法和套路可以用到类似的线性模型上，比如逻辑回归。 3.1 正则化模型正则化就是把额外的约束或者惩罚项加到已有模型（损失函数）上，以防止过拟合并提高泛化能力。损失函数由原来的E(X,Y)变为E(X,Y)+alpha||w||，w是模型系数组成的向量（有些地方也叫参数parameter，coefficients），||·||一般是L1或者L2范数，alpha是一个可调的参数，控制着正则化的强度。当用在线性模型上时，L1正则化和L2正则化也称为Lasso和Ridge。 3.2 L1正则化/LassoL1正则化将系数w的l1范数作为惩罚项加到损失函数上，由于正则项非零，这就迫使那些弱的特征所对应的系数变成0。因此L1正则化往往会使学到的模型很稀疏（系数w经常为0），这个特性使得L1正则化成为一种很好的特征选择方法。 Scikit-learn为线性回归提供了Lasso，为分类提供了L1逻辑回归。 下面的例子在波士顿房价数据上运行了Lasso，其中参数alpha是通过grid search进行优化的。 1234567891011121314from sklearn.linear_model import Lassofrom sklearn.preprocessing import StandardScalerfrom sklearn.datasets import load_bostonboston &#x3D; load_boston()scaler &#x3D; StandardScaler()X &#x3D; scaler.fit_transform(boston[&quot;data&quot;])Y &#x3D; boston[&quot;target&quot;]names &#x3D; boston[&quot;feature_names&quot;]lasso &#x3D; Lasso(alpha&#x3D;.3)lasso.fit(X, Y)print &quot;Lasso model: &quot;, pretty_print_linear(lasso.coef_, names, sort &#x3D; True) Lasso model: -3.707 * LSTAT + 2.992 * RM + -1.757 * PTRATIO + -1.081 * DIS + -0.7 * NOX + 0.631 * B + 0.54 * CHAS + -0.236 * CRIM + 0.081 * ZN + -0.0 * INDUS + -0.0 * AGE + 0.0 * RAD + -0.0 * TAX 可以看到，很多特征的系数都是0。如果继续增加alpha的值，得到的模型就会越来越稀疏，即越来越多的特征系数会变成0。 然而，L1正则化像非正则化线性模型一样也是不稳定的，如果特征集合中具有相关联的特征，当数据发生细微变化时也有可能导致很大的模型差异。 3.3 L2正则化/Ridge regressionL2正则化将系数向量的L2范数添加到了损失函数中。由于L2惩罚项中系数是二次方的，这使得L2和L1有着诸多差异，最明显的一点就是，L2正则化会让系数的取值变得平均。对于关联特征，这意味着他们能够获得更相近的对应系数。还是以Y=X1+X2为例，假设X1和X2具有很强的关联，如果用L1正则化，不论学到的模型是Y=X1+X2还是Y=2X1，惩罚都是一样的，都是2alpha。但是对于L2来说，第一个模型的惩罚项是2alpha，但第二个模型的是4*alpha。可以看出，系数之和为常数时，各系数相等时惩罚是最小的，所以才有了L2会让各个系数趋于相同的特点。 可以看出，L2正则化对于特征选择来说一种稳定的模型，不像L1正则化那样，系数会因为细微的数据变化而波动。所以L2正则化和L1正则化提供的价值是不同的，L2正则化对于特征理解来说更加有用：表示能力强的特征对应的系数是非零。 回过头来看看3个互相关联的特征的例子，分别以10个不同的种子随机初始化运行10次，来观察L1和L2正则化的稳定性。 123456789101112131415161718192021222324from sklearn.linear_model import Ridgefrom sklearn.metrics import r2_scoresize &#x3D; 100#We run the method 10 times with different random seedsfor i in range(10): print &quot;Random seed %s&quot; % i np.random.seed(seed&#x3D;i) X_seed &#x3D; np.random.normal(0, 1, size) X1 &#x3D; X_seed + np.random.normal(0, .1, size) X2 &#x3D; X_seed + np.random.normal(0, .1, size) X3 &#x3D; X_seed + np.random.normal(0, .1, size) Y &#x3D; X1 + X2 + X3 + np.random.normal(0, 1, size) X &#x3D; np.array([X1, X2, X3]).T lr &#x3D; LinearRegression() lr.fit(X,Y) print &quot;Linear model:&quot;, pretty_print_linear(lr.coef_) ridge &#x3D; Ridge(alpha&#x3D;10) ridge.fit(X,Y) print &quot;Ridge model:&quot;, pretty_print_linear(ridge.coef_) print Random seed 0 Linear model: 0.728 * X0 + 2.309 * X1 + -0.082 * X2 Ridge model: 0.938 * X0 + 1.059 * X1 + 0.877 * X2 Random seed 1 Linear model: 1.152 * X0 + 2.366 * X1 + -0.599 * X2 Ridge model: 0.984 * X0 + 1.068 * X1 + 0.759 * X2 Random seed 2 Linear model: 0.697 * X0 + 0.322 * X1 + 2.086 * X2 Ridge model: 0.972 * X0 + 0.943 * X1 + 1.085 * X2 Random seed 3 Linear model: 0.287 * X0 + 1.254 * X1 + 1.491 * X2 Ridge model: 0.919 * X0 + 1.005 * X1 + 1.033 * X2 Random seed 4 Linear model: 0.187 * X0 + 0.772 * X1 + 2.189 * X2 Ridge model: 0.964 * X0 + 0.982 * X1 + 1.098 * X2 Random seed 5 Linear model: -1.291 * X0 + 1.591 * X1 + 2.747 * X2 Ridge model: 0.758 * X0 + 1.011 * X1 + 1.139 * X2 Random seed 6 Linear model: 1.199 * X0 + -0.031 * X1 + 1.915 * X2 Ridge model: 1.016 * X0 + 0.89 * X1 + 1.091 * X2 Random seed 7 Linear model: 1.474 * X0 + 1.762 * X1 + -0.151 * X2 Ridge model: 1.018 * X0 + 1.039 * X1 + 0.901 * X2 Random seed 8 Linear model: 0.084 * X0 + 1.88 * X1 + 1.107 * X2 Ridge model: 0.907 * X0 + 1.071 * X1 + 1.008 * X2 Random seed 9 Linear model: 0.714 * X0 + 0.776 * X1 + 1.364 * X2 Ridge model: 0.896 * X0 + 0.903 * X1 + 0.98 * X2 可以看出，不同的数据上线性回归得到的模型（系数）相差甚远，但对于L2正则化模型来说，结果中的系数非常的稳定，差别较小，都比较接近于1，能够反映出数据的内在结构。 4 随机森林随机森林具有准确率高、鲁棒性好、易于使用等优点，这使得它成为了目前最流行的机器学习算法之一。随机森林提供了两种特征选择的方法：mean decrease impurity和mean decrease accuracy。 4.1 平均不纯度减少 mean decrease impurity随机森林由多个决策树构成。决策树中的每一个节点都是关于某个特征的条件，为的是将数据集按照不同的响应变量一分为二。利用不纯度可以确定节点（最优条件），对于分类问题，通常采用基尼不纯度或者信息增益，对于回归问题，通常采用的是方差或者最小二乘拟合。当训练决策树的时候，可以计算出每个特征减少了多少树的不纯度。对于一个决策树森林来说，可以算出每个特征平均减少了多少不纯度，并把它平均减少的不纯度作为特征选择的值。 下边的例子是sklearn中基于随机森林的特征重要度度量方法： 1234567891011121314from sklearn.datasets import load_bostonfrom sklearn.ensemble import RandomForestRegressorimport numpy as np#Load boston housing dataset as an exampleboston &#x3D; load_boston()X &#x3D; boston[&quot;data&quot;]Y &#x3D; boston[&quot;target&quot;]names &#x3D; boston[&quot;feature_names&quot;]rf &#x3D; RandomForestRegressor()rf.fit(X, Y)print &quot;Features sorted by their score:&quot;print sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), names), reverse&#x3D;True) Features sorted by their score: [(0.5298, ‘LSTAT’), (0.4116, ‘RM’), (0.0252, ‘DIS’), (0.0172, ‘CRIM’), (0.0065, ‘NOX’), (0.0035, ‘PTRATIO’), (0.0021, ‘TAX’), (0.0017, ‘AGE’), (0.0012, ‘B’), (0.0008, ‘INDUS’), (0.0004, ‘RAD’), (0.0001, ‘CHAS’), (0.0, ‘ZN’)] 这里特征得分实际上采用的是Gini Importance。使用基于不纯度的方法的时候，要记住：1、这种方法存在偏向，对具有更多类别的变量会更有利；2、对于存在关联的多个特征，其中任意一个都可以作为指示器（优秀的特征），并且一旦某个特征被选择之后，其他特征的重要度就会急剧下降，因为不纯度已经被选中的那个特征降下来了，其他的特征就很难再降低那么多不纯度了，这样一来，只有先被选中的那个特征重要度很高，其他的关联特征重要度往往较低。在理解数据时，这就会造成误解，导致错误的认为先被选中的特征是很重要的，而其余的特征是不重要的，但实际上这些特征对响应变量的作用确实非常接近的（这跟Lasso是很像的）。 特征随机选择方法稍微缓解了这个问题，但总的来说并没有完全解决。下面的例子中，X0、X1、X2是三个互相关联的变量，在没有噪音的情况下，输出变量是三者之和。 12345678910111213size &#x3D; 10000np.random.seed(seed&#x3D;10)X_seed &#x3D; np.random.normal(0, 1, size)X0 &#x3D; X_seed + np.random.normal(0, .1, size)X1 &#x3D; X_seed + np.random.normal(0, .1, size)X2 &#x3D; X_seed + np.random.normal(0, .1, size)X &#x3D; np.array([X0, X1, X2]).TY &#x3D; X0 + X1 + X2rf &#x3D; RandomForestRegressor(n_estimators&#x3D;20, max_features&#x3D;2)rf.fit(X, Y);print &quot;Scores for X0, X1, X2:&quot;, map(lambda x:round (x,3), rf.feature_importances_) Scores for X0, X1, X2: [0.278, 0.66, 0.062] 当计算特征重要性时，可以看到X1的重要度比X2的重要度要高出10倍，但实际上他们真正的重要度是一样的。尽管数据量已经很大且没有噪音，且用了20棵树来做随机选择，但这个问题还是会存在。 需要注意的一点是，关联特征的打分存在不稳定的现象，这不仅仅是随机森林特有的，大多数基于模型的特征选择方法都存在这个问题。 4.2 平均精确率减少 Mean decrease accuracy另一种常用的特征选择方法就是直接度量每个特征对模型精确率的影响。主要思路是打乱每个特征的特征值顺序，并且度量顺序变动对模型的精确率的影响。很明显，对于不重要的变量来说，打乱顺序对模型的精确率影响不会太大，但是对于重要的变量来说，打乱顺序就会降低模型的精确率。 这个方法sklearn中没有直接提供，但是很容易实现，下面继续在波士顿房价数据集上进行实现。 123456789101112131415161718192021222324from sklearn.cross_validation import ShuffleSplitfrom sklearn.metrics import r2_scorefrom collections import defaultdictX &#x3D; boston[&quot;data&quot;]Y &#x3D; boston[&quot;target&quot;]rf &#x3D; RandomForestRegressor()scores &#x3D; defaultdict(list)#crossvalidate the scores on a number of different random splits of the datafor train_idx, test_idx in ShuffleSplit(len(X), 100, .3): X_train, X_test &#x3D; X[train_idx], X[test_idx] Y_train, Y_test &#x3D; Y[train_idx], Y[test_idx] r &#x3D; rf.fit(X_train, Y_train) acc &#x3D; r2_score(Y_test, rf.predict(X_test)) for i in range(X.shape[1]): X_t &#x3D; X_test.copy() np.random.shuffle(X_t[:, i]) shuff_acc &#x3D; r2_score(Y_test, rf.predict(X_t)) scores[names[i]].append((acc-shuff_acc)&#x2F;acc)print &quot;Features sorted by their score:&quot;print sorted([(round(np.mean(score), 4), feat) for feat, score in scores.items()], reverse&#x3D;True) Features sorted by their score: [(0.7276, ‘LSTAT’), (0.5675, ‘RM’), (0.0867, ‘DIS’), (0.0407, ‘NOX’), (0.0351, ‘CRIM’), (0.0233, ‘PTRATIO’), (0.0168, ‘TAX’), (0.0122, ‘AGE’), (0.005, ‘B’), (0.0048, ‘INDUS’), (0.0043, ‘RAD’), (0.0004, ‘ZN’), (0.0001, ‘CHAS’)] 在这个例子当中，LSTAT和RM这两个特征对模型的性能有着很大的影响，打乱这两个特征的特征值使得模型的性能下降了73%和57%。注意，尽管这些我们是在所有特征上进行了训练得到了模型，然后才得到了每个特征的重要性测试，这并不意味着我们扔掉某个或者某些重要特征后模型的性能就一定会下降很多，因为即便某个特征删掉之后，其关联特征一样可以发挥作用，让模型性能基本上不变。 5 两种顶层特征选择算法之所以叫做顶层，是因为他们都是建立在基于模型的特征选择方法基础之上的，例如回归和SVM，在不同的子集上建立模型，然后汇总最终确定特征得分。 5.1 稳定性选择 Stability selection稳定性选择是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，比如可以统计某个特征被认为是重要特征的频率（被选为重要特征的次数除以它所在的子集被测试的次数）。理想情况下，重要特征的得分会接近100%。稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近于0。 sklearn在随机lasso和随机逻辑回归中有对稳定性选择的实现。 12345678910111213141516from sklearn.linear_model import RandomizedLassofrom sklearn.datasets import load_bostonboston &#x3D; load_boston()#using the Boston housing data. #Data gets scaled automatically by sklearn&#39;s implementationX &#x3D; boston[&quot;data&quot;]Y &#x3D; boston[&quot;target&quot;]names &#x3D; boston[&quot;feature_names&quot;]rlasso &#x3D; RandomizedLasso(alpha&#x3D;0.025)rlasso.fit(X, Y)print &quot;Features sorted by their score:&quot;print sorted(zip(map(lambda x: round(x, 4), rlasso.scores_), names), reverse&#x3D;True) Features sorted by their score: [(1.0, ‘RM’), (1.0, ‘PTRATIO’), (1.0, ‘LSTAT’), (0.62, ‘CHAS’), (0.595, ‘B’), (0.39, ‘TAX’), (0.385, ‘CRIM’), (0.25, ‘DIS’), (0.22, ‘NOX’), (0.125, ‘INDUS’), (0.045, ‘ZN’), (0.02, ‘RAD’), (0.015, ‘AGE’)] 在上边这个例子当中，最高的3个特征得分是1.0，这表示他们总会被选作有用的特征（当然，得分会收到正则化参数alpha的影响，但是sklearn的随机lasso能够自动选择最优的alpha）。接下来的几个特征得分就开始下降，但是下降的不是特别急剧，这跟纯lasso的方法和随机森林的结果不一样。能够看出稳定性选择对于克服过拟合和对数据理解来说都是有帮助的：总的来说，好的特征不会因为有相似的特征、关联特征而得分为0，这跟Lasso是不同的。对于特征选择任务，在许多数据集和环境下，稳定性选择往往是性能最好的方法之一。 5.2 递归特征消除 Recursive feature elimination (RFE)递归特征消除的主要思想是反复的构建模型（如SVM或者回归模型）然后选出最好的（或者最差的）的特征（可以根据系数来选），把选出来的特征放到一遍，然后在剩余的特征上重复这个过程，直到所有特征都遍历了。这个过程中特征被消除的次序就是特征的排序。因此，这是一种寻找最优特征子集的贪心算法。 RFE的稳定性很大程度上取决于在迭代的时候底层用哪种模型。例如，假如RFE采用的普通的回归，没有经过正则化的回归是不稳定的，那么RFE就是不稳定的；假如采用的是Ridge，而用Ridge正则化的回归是稳定的，那么RFE就是稳定的。 Sklearn提供了RFE包，可以用于特征消除，还提供了RFECV，可以通过交叉验证来对的特征进行排序。 12345678910111213141516from sklearn.feature_selection import RFEfrom sklearn.linear_model import LinearRegressionboston &#x3D; load_boston()X &#x3D; boston[&quot;data&quot;]Y &#x3D; boston[&quot;target&quot;]names &#x3D; boston[&quot;feature_names&quot;]#use linear regression as the modellr &#x3D; LinearRegression()#rank all features, i.e continue the elimination until the last onerfe &#x3D; RFE(lr, n_features_to_select&#x3D;1)rfe.fit(X,Y)print &quot;Features sorted by their rank:&quot;print sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), names)) Features sorted by their rank: [(1.0, ‘NOX’), (2.0, ‘RM’), (3.0, ‘CHAS’), (4.0, ‘PTRATIO’), (5.0, ‘DIS’), (6.0, ‘LSTAT’), (7.0, ‘RAD’), (8.0, ‘CRIM’), (9.0, ‘INDUS’), (10.0, ‘ZN’), (11.0, ‘TAX’), (12.0, ‘B’), (13.0, ‘AGE’)] 6 一个完整的例子下面将本文所有提到的方法进行实验对比，数据集采用Friedman #1 回归数据（这篇论文中的数据）。数据是用这个公式产生的： X1到X5是由单变量分布生成的，e是标准正态变量N(0,1)。另外，原始的数据集中含有5个噪音变量 X5,…,X10，跟响应变量是独立的。我们增加了4个额外的变量X11,…X14，分别是X1,…,X4的关联变量，通过f(x)=x+N(0,0.01)生成，这将产生大于0.999的关联系数。这样生成的数据能够体现出不同的特征排序方法应对关联特征时的表现。 接下来将会在上述数据上运行所有的特征选择方法，并且将每种方法给出的得分进行归一化，让取值都落在0-1之间。对于RFE来说，由于它给出的是顺序而不是得分，我们将最好的5个的得分定为1，其他的特征的得分均匀的分布在0-1之间。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384from sklearn.datasets import load_bostonfrom sklearn.linear_model import (LinearRegression, Ridge, Lasso, RandomizedLasso)from sklearn.feature_selection import RFE, f_regressionfrom sklearn.preprocessing import MinMaxScalerfrom sklearn.ensemble import RandomForestRegressorimport numpy as npfrom minepy import MINEnp.random.seed(0)size &#x3D; 750X &#x3D; np.random.uniform(0, 1, (size, 14))#&quot;Friedamn #1” regression problemY &#x3D; (10 * np.sin(np.pi*X[:,0]*X[:,1]) + 20*(X[:,2] - .5)**2 + 10*X[:,3] + 5*X[:,4] + np.random.normal(0,1))#Add 3 additional correlated variables (correlated with X1-X3)X[:,10:] &#x3D; X[:,:4] + np.random.normal(0, .025, (size,4))names &#x3D; [&quot;x%s&quot; % i for i in range(1,15)]ranks &#x3D; &#123;&#125;def rank_to_dict(ranks, names, order&#x3D;1): minmax &#x3D; MinMaxScaler() ranks &#x3D; minmax.fit_transform(order*np.array([ranks]).T).T[0] ranks &#x3D; map(lambda x: round(x, 2), ranks) return dict(zip(names, ranks ))lr &#x3D; LinearRegression(normalize&#x3D;True)lr.fit(X, Y)ranks[&quot;Linear reg&quot;] &#x3D; rank_to_dict(np.abs(lr.coef_), names)ridge &#x3D; Ridge(alpha&#x3D;7)ridge.fit(X, Y)ranks[&quot;Ridge&quot;] &#x3D; rank_to_dict(np.abs(ridge.coef_), names)lasso &#x3D; Lasso(alpha&#x3D;.05)lasso.fit(X, Y)ranks[&quot;Lasso&quot;] &#x3D; rank_to_dict(np.abs(lasso.coef_), names)rlasso &#x3D; RandomizedLasso(alpha&#x3D;0.04)rlasso.fit(X, Y)ranks[&quot;Stability&quot;] &#x3D; rank_to_dict(np.abs(rlasso.scores_), names)#stop the search when 5 features are left (they will get equal scores)rfe &#x3D; RFE(lr, n_features_to_select&#x3D;5)rfe.fit(X,Y)ranks[&quot;RFE&quot;] &#x3D; rank_to_dict(map(float, rfe.ranking_), names, order&#x3D;-1)rf &#x3D; RandomForestRegressor()rf.fit(X,Y)ranks[&quot;RF&quot;] &#x3D; rank_to_dict(rf.feature_importances_, names)f, pval &#x3D; f_regression(X, Y, center&#x3D;True)ranks[&quot;Corr.&quot;] &#x3D; rank_to_dict(f, names)mine &#x3D; MINE()mic_scores &#x3D; []for i in range(X.shape[1]): mine.compute_score(X[:,i], Y) m &#x3D; mine.mic() mic_scores.append(m)ranks[&quot;MIC&quot;] &#x3D; rank_to_dict(mic_scores, names)r &#x3D; &#123;&#125;for name in names: r[name] &#x3D; round(np.mean([ranks[method][name] for method in ranks.keys()]), 2)methods &#x3D; sorted(ranks.keys())ranks[&quot;Mean&quot;] &#x3D; rmethods.append(&quot;Mean&quot;)print &quot;\\t%s&quot; % &quot;\\t&quot;.join(methods)for name in names: print &quot;%s\\t%s&quot; % (name, &quot;\\t&quot;.join(map(str, [ranks[method][name] for method in methods]))) 从以上结果中可以找到一些有趣的发现： 特征之间存在线性关联关系，每个特征都是独立评价的，因此X1,…X4的得分和X11,…X14的得分非常接近，而噪音特征X5,…,X10正如预期的那样和响应变量之间几乎没有关系。由于变量X3是二次的，因此X3和响应变量之间看不出有关系（除了MIC之外，其他方法都找不到关系）。这种方法能够衡量出特征和响应变量之间的线性关系，但若想选出优质特征来提升模型的泛化能力，这种方法就不是特别给力了，因为所有的优质特征都不可避免的会被挑出来两次。 Lasso能够挑出一些优质特征，同时让其他特征的系数趋于0。当如需要减少特征数的时候它很有用，但是对于数据理解来说不是很好用。（例如在结果表中，X11,X12,X13的得分都是0，好像他们跟输出变量之间没有很强的联系，但实际上不是这样的） MIC对特征一视同仁，这一点上和关联系数有点像，另外，它能够找出X3和响应变量之间的非线性关系。 随机森林基于不纯度的排序结果非常鲜明，在得分最高的几个特征之后的特征，得分急剧的下降。从表中可以看到，得分第三的特征比第一的小4倍。而其他的特征选择算法就没有下降的这么剧烈。 Ridge将回归系数均匀的分摊到各个关联变量上，从表中可以看出，X11,…,X14和X1,…,X4的得分非常接近。 稳定性选择常常是一种既能够有助于理解数据又能够挑出优质特征的这种选择，在结果表中就能很好的看出。像Lasso一样，它能找到那些性能比较好的特征（X1，X2，X4，X5），同时，与这些特征关联度很强的变量也得到了较高的得分。 总结 对于理解数据、数据的结构、特点来说，单变量特征选择是个非常好的选择。尽管可以用它对特征进行排序来优化模型，但由于它不能发现冗余（例如假如一个特征子集，其中的特征之间具有很强的关联，那么从中选择最优的特征时就很难考虑到冗余的问题）。 正则化的线性模型对于特征理解和特征选择来说是非常强大的工具。L1正则化能够生成稀疏的模型，对于选择特征子集来说非常有用；相比起L1正则化，L2正则化的表现更加稳定，由于有用的特征往往对应系数非零，因此L2正则化对于数据的理解来说很合适。由于响应变量和特征之间往往是非线性关系，可以采用basis expansion的方式将特征转换到一个更加合适的空间当中，在此基础上再考虑运用简单的线性模型。 随机森林是一种非常流行的特征选择方法，它易于使用，一般不需要feature engineering、调参等繁琐的步骤，并且很多工具包都提供了平均不纯度下降方法。它的两个主要问题，1是重要的特征有可能得分很低（关联特征问题），2是这种方法对特征变量类别多的特征越有利（偏向问题）。尽管如此，这种方法仍然非常值得在你的应用中试一试。 特征选择在很多机器学习和数据挖掘场景中都是非常有用的。在使用的时候要弄清楚自己的目标是什么，然后找到哪种方法适用于自己的任务。当选择最优特征以提升模型性能的时候，可以采用交叉验证的方法来验证某种方法是否比其他方法要好。当用特征选择的方法来理解数据的时候要留心，特征选择模型的稳定性非常重要，稳定性差的模型很容易就会导致错误的结论。对数据进行二次采样然后在子集上运行特征选择算法能够有所帮助，如果在各个子集上的结果是一致的，那就可以说在这个数据集上得出来的结论是可信的，可以用这种特征选择模型的结果来理解数据。 Tips什么是卡方检验？用方差来衡量某个观测频率和理论频率之间差异性的方法 什么是皮尔森卡方检验？这是一种最常用的卡方检验方法，它有两个用途：1是计算某个变量对某种分布的拟合程度，2是根据两个观测变量的Contingency table来计算这两个变量是否是独立的。主要有三个步骤：第一步用方差和的方式来计算观测频率和理论频率之间卡方值；第二步算出卡方检验的自由度（行数-1乘以列数-1）；第三步比较卡方值和对应自由度的卡方分布，判断显著性。 什么是p-value？简单地说，p-value就是为了验证假设和实际之间一致性的统计学意义的值，即假设检验。有些地方叫右尾概率，根据卡方值和自由度可以算出一个固定的p-value， 什么是响应变量(response value)？简单地说，模型的输入叫做explanatroy variables，模型的输出叫做response variables，其实就是要验证该特征对结果造成了什么样的影响 什么是统计能力(statistical power)? 什么是度量(metric)? 什么是零假设(null hypothesis)?在相关性检验中，一般会取“两者之间无关联”作为零假设，而在独立性检验中，一般会取“两者之间是独立”作为零假设。与零假设相对的是备择假设（对立假设），即希望证明是正确的另一种可能。 什么是多重共线性？ 什么是grid search？","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://coolwin8.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"特征选择","slug":"特征选择","permalink":"http://coolwin8.github.io/tags/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"},{"name":"网文精选","slug":"网文精选","permalink":"http://coolwin8.github.io/tags/%E7%BD%91%E6%96%87%E7%B2%BE%E9%80%89/"}]},{"title":"半监督学习   semi-supervisered","slug":"半监督学习","date":"2020-04-11T06:11:56.574Z","updated":"2020-04-13T01:34:14.558Z","comments":true,"path":"2020/04/11/半监督学习/","link":"","permalink":"http://coolwin8.github.io/2020/04/11/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"标注数据和未标注数据 直推 内部预测 归纳式 SSL 主动学习 机器先对未标注数据进行标注，人对机器不太确认的部分进行标注（p值），再把人标注数据放入训练集训练模型 SSL并非都有效 非标签数据加入可能不如加入少量标签数据； 假设满足 假设分布 同分布下同类样本距离更近 半监督趋势 边界 密度 聚类 自学习 self -learning 生成模型和判别模型 条件概率和联合概率 Multi-View Learning / Co-Learning 标签传递","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://coolwin8.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"半监督","slug":"半监督","permalink":"http://coolwin8.github.io/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3/"},{"name":"机器学习","slug":"机器学习","permalink":"http://coolwin8.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"通用流程","slug":"通用流程","date":"2020-04-11T06:11:56.567Z","updated":"2019-11-26T07:01:36.000Z","comments":true,"path":"2020/04/11/通用流程/","link":"","permalink":"http://coolwin8.github.io/2020/04/11/%E9%80%9A%E7%94%A8%E6%B5%81%E7%A8%8B/","excerpt":"","text":"机器学习业务通用流程1、数据清洗 缺失值 id 常数列 2、类型标准化 数值列 int32 float32 类别列 object 日期列 统一格式 输入： dict({dataDf / evalDf}) dict(params) 输出： Df 3、数据处理-缺失值处理 输入： Df1 Map（col: method) 输出： Df1 4、数据探查profile KS检验 检验两个数据集是否同分布（分别检验多种不同分布） 5、行缺失6、删除列（reference ） 输入： Df1 ，Df2(reference) 7、异常值处理 两种方式： u+3delta / 分位数+1.5倍 数据处理 拆分训练集、测试集 拆分数据类型： 时间、数值 时间变类别 连续便离散 拼表 类别列编码 应用连续到离散分箱器 应用连续到离散编码器 拼表 缺失率阈值计算 通过计算能力确定阈值，如以最终可用列数来约束 通过列缺失率分布来确定异常值的阈值，作为缺失率阈值 根据模型评估的结果来调整缺失率阈值，以确定保留的列 模型训练","categories":[],"tags":[]},{"title":"部署PyPi离线私有库","slug":"部署PyPi离线私有库","date":"2020-04-11T02:58:52.450Z","updated":"2020-04-13T01:34:54.401Z","comments":true,"path":"2020/04/11/部署PyPi离线私有库/","link":"","permalink":"http://coolwin8.github.io/2020/04/11/%E9%83%A8%E7%BD%B2PyPi%E7%A6%BB%E7%BA%BF%E7%A7%81%E6%9C%89%E5%BA%93/","excerpt":"","text":"1. 拉取镜像 pypiserver/pypiserver1docker pull pypiserver&#x2F;pypiserver:latest 2. docker 部署运行 运行container1234567891011mkdir -p &#x2F;data&#x2F;pypi&#x2F;packages# 无认证目录上传，直接将安装包下载到映射目录即可docker run --name pypiserver --restart&#x3D;always -d -p 8088:8080 -v &#x2F;data&#x2F;pypi&#x2F;packages:&#x2F;data&#x2F;packages pypiserver&#x2F;pypiserver:latest# apache认证pypiupload 上传htpasswd -sc &#x2F;data&#x2F;pypi&#x2F;auth&#x2F;.htpasswd lengweiln -sv &#x2F;usr&#x2F;local&#x2F;sbin&#x2F;python3.7&#x2F;bin&#x2F;pypiupload &#x2F;us&#x2F;bin&#x2F;pypiuploaddocker run --name pypiserver -d -p 8088:8080 -v &#x2F;data&#x2F;pypi&#x2F;auth&#x2F;.htpasswd:&#x2F;data&#x2F;.htpasswd pypiserver&#x2F;pypiserver:latest -P .htpasswd packages# 合成docker run --name pypiserver --restart&#x3D;always -d -p 8088:8080 -v &#x2F;data&#x2F;pypi&#x2F;auth&#x2F;.htpasswd:&#x2F;data&#x2F;.htpasswd -v &#x2F;data&#x2F;pypi&#x2F;packages&#x2F;:&#x2F;data&#x2F;packages&#x2F; pypiserver&#x2F;pypiserver:latest -P .htpasswd packages 3. 客户端配置 配置pypi 123456789[root@localhost pypi]# vi ~&#x2F;.pypirc [distutils]index-servers &#x3D; local[local]repository: http:&#x2F;&#x2F;192.168.1.14:8088username: lengweipassword: 123456 安装上传工具 pypi-upload 12[root@localhost]# pip install pypi-uploadpypiupload files &#x2F;data&#x2F;pypi&#x2F;packages&#x2F;* -i local 配置pip 12345678[root@localhost pypi]# vi ~&#x2F;.pip&#x2F;pip.conf[global]index-url &#x3D; http:&#x2F;&#x2F;192.168.1.14&#x2F;simpleextra-index-url&#x3D;https:&#x2F;&#x2F;pypi.mirrors.ustc.edu.cn&#x2F;simple# aliyun pypi镜像 https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;pypi&#x2F;simple&#x2F;[install]trusted-host &#x3D; 192.168.1.14 4. 拉取依赖包到私有库 (在不同的平台下执行拉取，如win_amd64、manylinux等) Python 项目使用 pip 安装的包，都可以通过 pip freeze &gt;requirements.txt 导出环境中已有的模块。搭建 requirements.txt 离线 PyPI 仓库，我们首先需要把 requirements.txt 所有的模块安装包下载到本地。 1234567891011121314$ pip download -d &#x2F;data&#x2F;pypi&#x2F;packages -r requirements.txt --index-url https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;pypi&#x2F;simple --extra-index-url https:&#x2F;&#x2F;wheels.galaxyproject.org&#x2F;simpleLooking in indexes: https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;pypi&#x2F;simple, https:&#x2F;&#x2F;wheels.galaxyproject.org&#x2F;simpleCollecting amqp&#x3D;&#x3D;2.2.2 (from -r &#x2F;&#x2F;home&#x2F;shenweiyan&#x2F;galaxy&#x2F;requirements.txt (line 1))Downloading https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;pypi&#x2F;packages&#x2F;88&#x2F;4a&#x2F;8c45a882d842678963516ebd9cf584a4ded51af719234c3b696c2e884c60&#x2F;amqp-2.2.2-py2.py3-none-any.whl (48kB) 100% |████████████████████████████████| 51kB 779kB&#x2F;sSaved .&#x2F;amqp-2.2.2-py2.py3-none-any.whl......Collecting wrapt&#x3D;&#x3D;1.10.11 (from -r &#x2F;home&#x2F;shenweiyan&#x2F;galaxy&#x2F;requirements.txt (line 134))Downloading https:&#x2F;&#x2F;wheels.galaxyproject.org&#x2F;packages&#x2F;wrapt-1.10.11-cp27-cp27mu-manylinux1_x86_64.whl (64kB) 100% |████████████████████████████████| 71kB 321kB&#x2F;sSaved &#x2F;home&#x2F;galaxy&#x2F;packages&#x2F;wrapt-1.10.11-cp27-cp27mu-manylinux1_x86_64.whlSuccessfully downloaded amqp appdirs asn1crypto babel bagit bcrypt bdbag beaker bioblend bleach boltons boto bunch bx-python bz2file certifi ...... wcwidth webencodings webob whoosh wrapt 把 /home/shenweiyan/packages 整个目录拷贝到目标服务器(可连网但速度极慢，目标路径：/data/galaxy-dist/packages)，搭建并启动 pypiserver，然后从本地离线 PyPI 仓库安装 requirements 软件 1$ pip install --index-url http:&#x2F;&#x2F;localhost:8080&#x2F;simple&#x2F; --extra-index-url https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;pypi&#x2F;simple&#x2F; -r requirements.txt 5. 获取现有依赖库 pip导出安装的库到requirements.txt 1pip freeze &gt; requirements.txt pip导入requirements.txt中列出的库到系统 12pip install -r requirements.txtpip download -d &#x2F;data&#x2F;pypi&#x2F;packages -r requirements.txt pip download –only-binary=:all: –platform linux_x86_64 –python-version 3 –implementation cp –abi cp37m \\","categories":[{"name":"python","slug":"python","permalink":"http://coolwin8.github.io/categories/python/"},{"name":"pypi","slug":"python/pypi","permalink":"http://coolwin8.github.io/categories/python/pypi/"}],"tags":[{"name":"pypi","slug":"pypi","permalink":"http://coolwin8.github.io/tags/pypi/"},{"name":"部署","slug":"部署","permalink":"http://coolwin8.github.io/tags/%E9%83%A8%E7%BD%B2/"}]},{"title":"Hello World","slug":"hello-world","date":"2020-03-26T06:30:10.543Z","updated":"2020-04-11T06:10:48.416Z","comments":true,"path":"2020/03/26/hello-world/","link":"","permalink":"http://coolwin8.github.io/2020/03/26/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"系统应用 网络应用","slug":"系统应用-网络应用","permalink":"http://coolwin8.github.io/categories/%E7%B3%BB%E7%BB%9F%E5%BA%94%E7%94%A8-%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8/"},{"name":"java maven","slug":"java-maven","permalink":"http://coolwin8.github.io/categories/java-maven/"},{"name":"机器学习","slug":"机器学习","permalink":"http://coolwin8.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"python","slug":"python","permalink":"http://coolwin8.github.io/categories/python/"},{"name":"pypi","slug":"python/pypi","permalink":"http://coolwin8.github.io/categories/python/pypi/"}],"tags":[{"name":"网络配置","slug":"网络配置","permalink":"http://coolwin8.github.io/tags/%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE/"},{"name":"VPN","slug":"VPN","permalink":"http://coolwin8.github.io/tags/VPN/"},{"name":"内网访问","slug":"内网访问","permalink":"http://coolwin8.github.io/tags/%E5%86%85%E7%BD%91%E8%AE%BF%E9%97%AE/"},{"name":"配置管理","slug":"配置管理","permalink":"http://coolwin8.github.io/tags/%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86/"},{"name":"nexus","slug":"nexus","permalink":"http://coolwin8.github.io/tags/nexus/"},{"name":"maven","slug":"maven","permalink":"http://coolwin8.github.io/tags/maven/"},{"name":"特征选择","slug":"特征选择","permalink":"http://coolwin8.github.io/tags/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"},{"name":"网文精选","slug":"网文精选","permalink":"http://coolwin8.github.io/tags/%E7%BD%91%E6%96%87%E7%B2%BE%E9%80%89/"},{"name":"半监督","slug":"半监督","permalink":"http://coolwin8.github.io/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3/"},{"name":"机器学习","slug":"机器学习","permalink":"http://coolwin8.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"pypi","slug":"pypi","permalink":"http://coolwin8.github.io/tags/pypi/"},{"name":"部署","slug":"部署","permalink":"http://coolwin8.github.io/tags/%E9%83%A8%E7%BD%B2/"}]}